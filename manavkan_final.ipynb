{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f77c2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "CACHE REPLACEMENT RL - TRAINING\n",
      "Device: cpu\n",
      "Cache: 8 | Memory: 64 | Seq: 100\n",
      "============================================================\n",
      "Ep 50/700 | Reward (last 50): -10.36 | Hit: 44.82% | ε: 0.901\n",
      "Ep 100/700 | Reward (last 50): -10.96 | Hit: 44.52% | ε: 0.802\n",
      "Ep 150/700 | Reward (last 50): -13.60 | Hit: 43.20% | ε: 0.703\n",
      "Ep 200/700 | Reward (last 50): -9.32 | Hit: 45.34% | ε: 0.604\n",
      "Ep 250/700 | Reward (last 50): -10.56 | Hit: 44.72% | ε: 0.505\n",
      "Ep 300/700 | Reward (last 50): -6.92 | Hit: 46.54% | ε: 0.406\n",
      "Ep 350/700 | Reward (last 50): -4.00 | Hit: 48.00% | ε: 0.307\n",
      "Ep 400/700 | Reward (last 50): -1.16 | Hit: 49.42% | ε: 0.208\n",
      "Ep 450/700 | Reward (last 50): 3.40 | Hit: 51.70% | ε: 0.109\n",
      "Ep 500/700 | Reward (last 50): 3.08 | Hit: 51.54% | ε: 0.010\n",
      "Ep 550/700 | Reward (last 50): 2.84 | Hit: 51.42% | ε: 0.010\n",
      "Ep 600/700 | Reward (last 50): 3.68 | Hit: 51.84% | ε: 0.010\n",
      "Ep 650/700 | Reward (last 50): 8.44 | Hit: 54.22% | ε: 0.010\n",
      "Ep 700/700 | Reward (last 50): 4.24 | Hit: 52.12% | ε: 0.010\n",
      "Ep 50/700 | Reward (last 50): -11.08 | Hit: 44.46% | ε: 0.901\n",
      "Ep 100/700 | Reward (last 50): -8.96 | Hit: 45.52% | ε: 0.802\n",
      "Ep 150/700 | Reward (last 50): -10.24 | Hit: 44.88% | ε: 0.703\n",
      "Ep 200/700 | Reward (last 50): -6.88 | Hit: 46.56% | ε: 0.604\n",
      "Ep 250/700 | Reward (last 50): -7.76 | Hit: 46.12% | ε: 0.505\n",
      "Ep 300/700 | Reward (last 50): -4.48 | Hit: 47.76% | ε: 0.406\n",
      "Ep 350/700 | Reward (last 50): -2.72 | Hit: 48.64% | ε: 0.307\n",
      "Ep 400/700 | Reward (last 50): -1.68 | Hit: 49.16% | ε: 0.208\n",
      "Ep 450/700 | Reward (last 50): -2.16 | Hit: 48.92% | ε: 0.109\n",
      "Ep 500/700 | Reward (last 50): -2.20 | Hit: 48.90% | ε: 0.010\n",
      "Ep 550/700 | Reward (last 50): 0.80 | Hit: 50.40% | ε: 0.010\n",
      "Ep 600/700 | Reward (last 50): 2.76 | Hit: 51.38% | ε: 0.010\n",
      "Ep 650/700 | Reward (last 50): 1.72 | Hit: 50.86% | ε: 0.010\n",
      "Ep 700/700 | Reward (last 50): 4.28 | Hit: 52.14% | ε: 0.010\n",
      "\n",
      "EASY Pattern:\n",
      " LRU:  92.00% ± 0.00%\n",
      " FIFO: 92.00% ± 0.00%\n",
      " LFU:  92.00% ± 0.00%\n",
      " DQN:  92.00% ± 0.00%  ✗ (+0.00%)\n",
      " DDQN: 92.00% ± 0.00%  ✗ (+0.00%)\n",
      "\n",
      "MEDIUM Pattern:\n",
      " LRU:  28.40% ± 4.04%\n",
      " FIFO: 27.40% ± 3.84%\n",
      " LFU:  34.60% ± 6.51%\n",
      " DQN:  33.60% ± 4.42%  ✗ (-1.00%)\n",
      " DDQN: 31.80% ± 3.33%  ✗ (-2.80%)\n",
      "\n",
      "HARD Pattern:\n",
      " LRU:  42.10% ± 4.48%\n",
      " FIFO: 41.50% ± 5.11%\n",
      " LFU:  25.55% ± 4.54%\n",
      " DQN:  36.85% ± 5.03%  ✗ (-5.25%)\n",
      " DDQN: 38.20% ± 4.18%  ✗ (-3.90%)\n",
      "\n",
      "MIXED Pattern:\n",
      " LRU:  48.20% ± 5.52%\n",
      " FIFO: 44.85% ± 6.34%\n",
      " LFU:  56.65% ± 5.61%\n",
      " DQN:  53.05% ± 4.44%  ✗ (-3.60%)\n",
      " DDQN: 53.00% ± 6.92%  ✗ (-3.65%)\n",
      "✓ Results saved to 'cache_final_results.png'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nReferences\\n\\nhttps://www.cs.utexas.edu/~lin/papers/micro19c.pdf\\n\\nhttps://www.cs.cmu.edu/~weinaw/pdf/delayed-hits.pdf\\n\\nhttps://gymnasium.farama.org/index.html\\n\\nhttps://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final_caching.py\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque, OrderedDict, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Reproducible seeds\n",
    "\n",
    "GLOBAL_SEED = 42\n",
    "\n",
    "random.seed(GLOBAL_SEED)\n",
    "\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "\n",
    "torch.manual_seed(GLOBAL_SEED)\n",
    "\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(GLOBAL_SEED)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CacheEnv(gym.Env):\n",
    "    \n",
    "    \n",
    "    def __init__(self, cache_size=8, memory_size=64, sequence_length=100):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.cache_size = cache_size\n",
    "        self.memory_size = memory_size\n",
    "        \n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "\n",
    "        self.cache = OrderedDict()\n",
    "        self.access_sequence = []\n",
    "        \n",
    "        self.fixed_sequences = {}\n",
    "        self.current_step = 0\n",
    "\n",
    "\n",
    "        self.action_space = spaces.Discrete(cache_size)\n",
    "        \n",
    "        # state: cache contents (cache_size), freq (cache_size), recency (cache_size), next_addr, 3 future addrs\n",
    "        \n",
    "        self.observation_space = spaces.Box(low=-1.0, high=float(memory_size),\n",
    "                                           shape=(cache_size * 3 + 4,), dtype=np.float32)\n",
    "\n",
    "\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "        \n",
    "        self.total_accesses = 0\n",
    "        \n",
    "        self.access_freq = defaultdict(int)\n",
    "        self.last_access_time = {}\n",
    "\n",
    "        self._generate_fixed_sequences()\n",
    "\n",
    "\n",
    "\n",
    "    def _generate_fixed_sequences(self):\n",
    "        \n",
    "        for pattern in ['easy', 'medium', 'hard', 'mixed']:\n",
    "            \n",
    "            self.fixed_sequences[pattern] = []\n",
    "            \n",
    "            for i in range(50):\n",
    "                \n",
    "                self.fixed_sequences[pattern].append(self._generate_access_pattern(pattern, seed=i))\n",
    "\n",
    "\n",
    "\n",
    "    def _generate_access_pattern(self, pattern_type='mixed', seed=None):\n",
    "        \n",
    "        if seed is not None:\n",
    "            \n",
    "            old_state = random.getstate()\n",
    "            random.seed(seed)\n",
    "\n",
    "        seq = []\n",
    "        \n",
    "        if pattern_type == 'easy':\n",
    "            # loop size should not exceed cache capacity\n",
    "            loop_size = max(1, self.cache_size)\n",
    "            seq = [i % loop_size for i in range(self.sequence_length)]\n",
    "            \n",
    "        elif pattern_type == 'sequential':\n",
    "            seq = [i % self.memory_size for i in range(self.sequence_length)]\n",
    "            \n",
    "        elif pattern_type == 'loop':\n",
    "            loop_size = min(10, self.memory_size)\n",
    "            seq = [i % loop_size for i in range(self.sequence_length)]\n",
    "            \n",
    "        elif pattern_type == 'random':\n",
    "            seq = [random.randint(0, self.memory_size - 1) for _ in range(self.sequence_length)]\n",
    "            \n",
    "        elif pattern_type == 'hard':\n",
    "            seq = []\n",
    "            phases = max(1, self.sequence_length // 25)\n",
    "            for _ in range(phases):\n",
    "                phase_set = random.sample(range(self.memory_size), min(15, self.memory_size))\n",
    "                for _ in range(25):\n",
    "                    seq.append(random.choice(phase_set))\n",
    "            seq = seq[:self.sequence_length]\n",
    "            \n",
    "        elif pattern_type == 'medium':\n",
    "            \n",
    "            hot_set = list(range(min(8, self.memory_size)))\n",
    "            \n",
    "            warm_set = list(range(min(8, self.memory_size), min(20, self.memory_size)))\n",
    "            \n",
    "            cold_set = list(range(min(20, self.memory_size), self.memory_size))\n",
    "            \n",
    "            \n",
    "            for _ in range(self.sequence_length):\n",
    "                \n",
    "                r = random.random()\n",
    "                \n",
    "                if r < 0.5 and hot_set:\n",
    "                    seq.append(random.choice(hot_set))\n",
    "                elif r < 0.8 and warm_set:\n",
    "                    seq.append(random.choice(warm_set))\n",
    "                else:\n",
    "                    seq.append(random.choice(cold_set) if cold_set else random.randint(0, self.memory_size - 1))\n",
    "                    \n",
    "        else:  # mixed\n",
    "            \n",
    "            hot_set = list(range(min(6, self.memory_size)))\n",
    "            \n",
    "            warm_set = list(range(min(6, self.memory_size), min(12, self.memory_size)))\n",
    "            \n",
    "            cold_set = list(range(min(12, self.memory_size), self.memory_size))\n",
    "            \n",
    "            for _ in range(self.sequence_length):\n",
    "                \n",
    "                r = random.random()\n",
    "                \n",
    "                if r < 0.6 and hot_set:\n",
    "                    seq.append(random.choice(hot_set))\n",
    "                    \n",
    "                elif r < 0.85 and warm_set:\n",
    "                    seq.append(random.choice(warm_set))\n",
    "                    \n",
    "                else:\n",
    "                    seq.append(random.choice(cold_set) if cold_set else random.randint(0, self.memory_size - 1))\n",
    "                    \n",
    "                    \n",
    "\n",
    "        if seed is not None:\n",
    "            random.setstate(old_state)\n",
    "            \n",
    "            \n",
    "        return seq\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _get_state(self):\n",
    "        \n",
    "        cache_contents = list(self.cache.keys())\n",
    "        cache_contents = cache_contents[:self.cache_size]\n",
    "        \n",
    "        cache_contents += [-1] * (self.cache_size - len(cache_contents))\n",
    "\n",
    "\n",
    "\n",
    "        frequencies = []\n",
    "        \n",
    "        \n",
    "        recencies = []\n",
    "        \n",
    "        \n",
    "        for addr in cache_contents:\n",
    "            \n",
    "            if addr == -1:\n",
    "                \n",
    "                frequencies.append(0.0)\n",
    "                recencies.append(0.0)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                freq = self.access_freq.get(addr, 0) / 10.0\n",
    "                \n",
    "                recency = (self.current_step - self.last_access_time.get(addr, self.current_step)) / 50.0\n",
    "                \n",
    "                frequencies.append(min(freq, 1.0))\n",
    "                \n",
    "                recencies.append(min(recency, 1.0))\n",
    "\n",
    "\n",
    "\n",
    "        next_addr = -1\n",
    "        \n",
    "        if self.current_step < len(self.access_sequence):\n",
    "            \n",
    "            next_addr = self.access_sequence[self.current_step]\n",
    "            \n",
    "            \n",
    "\n",
    "        future_addrs = []\n",
    "        \n",
    "        \n",
    "        for i in range(1, 4):\n",
    "            \n",
    "            idx = self.current_step + i\n",
    "            \n",
    "            if idx < len(self.access_sequence):\n",
    "                future_addrs.append(self.access_sequence[idx])\n",
    "            else:\n",
    "                future_addrs.append(-1)\n",
    "\n",
    "\n",
    "\n",
    "        state = cache_contents + frequencies + recencies + [next_addr] + future_addrs\n",
    "        return np.array(state, dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \n",
    "        # choose fixed sequence if requested, otherwise generate a random one\n",
    "        \n",
    "        if options and 'sequence_idx' in options:\n",
    "            \n",
    "            pattern = options.get('pattern', 'mixed')\n",
    "            seq_idx = int(options['sequence_idx'])\n",
    "            seq_list = self.fixed_sequences.get(pattern, [])\n",
    "            self.access_sequence = seq_list[seq_idx % len(seq_list)]\n",
    "            \n",
    "        else:\n",
    "            pattern = options.get('pattern', 'mixed') if options else 'mixed'\n",
    "            self.access_sequence = self._generate_access_pattern(pattern)\n",
    "\n",
    "\n",
    "\n",
    "        # clear internal state\n",
    "        \n",
    "        self.cache = OrderedDict()\n",
    "        self.current_step = 0\n",
    "        \n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "        \n",
    "        self.total_accesses = 0\n",
    "        self.access_freq = defaultdict(int)\n",
    "        self.last_access_time = {}\n",
    "        \n",
    "        \n",
    "        return self._get_state(), {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        \n",
    "        if self.current_step >= len(self.access_sequence):\n",
    "            \n",
    "            return self._get_state(), 0.0, True, False, {}\n",
    "\n",
    "\n",
    "\n",
    "        addr = self.access_sequence[self.current_step]\n",
    "        self.access_freq[addr] += 1\n",
    "        \n",
    "        self.last_access_time[addr] = self.current_step\n",
    "\n",
    "\n",
    "        reward = 0.0\n",
    "        \n",
    "        if addr in self.cache:\n",
    "            self.hits += 1\n",
    "            reward = 1.0\n",
    "            self.cache.move_to_end(addr)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self.misses += 1\n",
    "            reward = -1.0\n",
    "            \n",
    "            if len(self.cache) >= self.cache_size:\n",
    "                cache_list = list(self.cache.keys())\n",
    "                \n",
    "                if 0 <= action < len(cache_list):\n",
    "                    evict_addr = cache_list[action]\n",
    "                    del self.cache[evict_addr]\n",
    "                else:\n",
    "                    self.cache.popitem(last=False)\n",
    "                    \n",
    "            self.cache[addr] = 1\n",
    "\n",
    "\n",
    "\n",
    "        self.total_accesses += 1\n",
    "        self.current_step += 1\n",
    "        \n",
    "        done = self.current_step >= len(self.access_sequence)\n",
    "        \n",
    "        return self._get_state(), float(reward), done, False, {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Baseline policies\n",
    "\n",
    "class LRUPolicy:\n",
    "    \n",
    "    def __init__(self, cache_size):\n",
    "        \n",
    "        self.cache = OrderedDict()\n",
    "        self.cache_size = cache_size\n",
    "\n",
    "\n",
    "\n",
    "    def access(self, addr):\n",
    "        \n",
    "        hit = addr in self.cache\n",
    "        \n",
    "        if hit:\n",
    "            \n",
    "            self.cache.move_to_end(addr)\n",
    "            return True\n",
    "        \n",
    "        if len(self.cache) >= self.cache_size:\n",
    "            \n",
    "            self.cache.popitem(last=False)\n",
    "            \n",
    "        self.cache[addr] = 1\n",
    "        \n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.cache = OrderedDict()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class FIFOPolicy:\n",
    "    \n",
    "    def __init__(self, cache_size):\n",
    "        \n",
    "        self.cache = OrderedDict()\n",
    "        self.cache_size = cache_size\n",
    "\n",
    "\n",
    "    def access(self, addr):\n",
    "        \n",
    "        if addr in self.cache:\n",
    "            return True\n",
    "        if len(self.cache) >= self.cache_size:\n",
    "            self.cache.popitem(last=False)\n",
    "            \n",
    "        self.cache[addr] = 1\n",
    "        \n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.cache = OrderedDict()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LFUPolicy:\n",
    "    \n",
    "    def __init__(self, cache_size):\n",
    "        \n",
    "        self.cache = {}\n",
    "        self.freq = {}\n",
    "        \n",
    "        self.cache_size = cache_size\n",
    "        \n",
    "        \n",
    "\n",
    "    def access(self, addr):\n",
    "        \n",
    "        if addr in self.cache:\n",
    "            self.freq[addr] += 1\n",
    "            return True\n",
    "        \n",
    "        if len(self.cache) >= self.cache_size:\n",
    "            \n",
    "            min_addr = min(self.freq, key=self.freq.get)\n",
    "            del self.cache[min_addr]\n",
    "            del self.freq[min_addr]\n",
    "            \n",
    "            \n",
    "        self.cache[addr] = 1\n",
    "        \n",
    "        self.freq[addr] = 1\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.cache = {}\n",
    "        self.freq = {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Networks and agents\n",
    "\n",
    "class DQNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=128):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        \n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    \n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "        \n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(tuple(args))\n",
    "        \n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, learning_rate=1e-3, gamma=0.95,\n",
    "                 epsilon_start=1.0, epsilon_end=0.01, epsilon_decay_steps=50000,\n",
    "                 replay_memory_size=50000, batch_size=64, target_update_freq=500):\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        self.gamma = gamma\n",
    "\n",
    "\n",
    "\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        \n",
    "        \n",
    "        self.epsilon_decay_steps = epsilon_decay_steps\n",
    "\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.target_update_freq = target_update_freq\n",
    "        \n",
    "\n",
    "        self.q_network = DQNetwork(state_dim, action_dim).to(self.device)\n",
    "        \n",
    "        self.target_network = DQNetwork(state_dim, action_dim).to(self.device)\n",
    "        \n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        self.target_network.eval()\n",
    "        \n",
    "\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.replay_memory = ReplayMemory(replay_memory_size)\n",
    "        \n",
    "        self.steps = 0\n",
    "\n",
    "\n",
    "\n",
    "    def select_action(self, state, evaluation=False):\n",
    "        \n",
    "        if evaluation or random.random() > self.epsilon:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                st = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "                q = self.q_network(st)\n",
    "                \n",
    "                return int(q.argmax().item())\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            return random.randrange(self.action_dim)\n",
    "\n",
    "\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.replay_memory.push(state, action, reward, next_state, float(done))\n",
    "\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        self.steps += 1\n",
    "        \n",
    "        fraction = min(1.0, self.steps / max(1, self.epsilon_decay_steps))\n",
    "        self.epsilon = self.epsilon_start + fraction * (self.epsilon_end - self.epsilon_start)\n",
    "        \n",
    "\n",
    "    def optimize_model(self):\n",
    "        \n",
    "        if len(self.replay_memory) < self.batch_size:\n",
    "            return None\n",
    "        transitions = self.replay_memory.sample(self.batch_size)\n",
    "        s, a, r, s2, d = zip(*transitions)\n",
    "        \n",
    "        \n",
    "        state_b = torch.FloatTensor(np.array(s)).to(self.device)\n",
    "        action_b = torch.LongTensor(a).unsqueeze(1).to(self.device)\n",
    "        \n",
    "        reward_b = torch.FloatTensor(r).unsqueeze(1).to(self.device)\n",
    "        \n",
    "        next_b = torch.FloatTensor(np.array(s2)).to(self.device)\n",
    "        done_b = torch.FloatTensor(d).unsqueeze(1).to(self.device)\n",
    "\n",
    "\n",
    "        current_q = self.q_network(state_b).gather(1, action_b)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            max_next_q = self.target_network(next_b).max(1)[0].unsqueeze(1)\n",
    "            target_q = reward_b + (1 - done_b) * self.gamma * max_next_q\n",
    "\n",
    "\n",
    "        loss = nn.MSELoss()(current_q, target_q)\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return float(loss.item())\n",
    "\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DoubleDQNAgent(DQNAgent):\n",
    "    \n",
    "    def optimize_model(self):\n",
    "        \n",
    "        if len(self.replay_memory) < self.batch_size:\n",
    "            \n",
    "            return None\n",
    "        \n",
    "        \n",
    "        transitions = self.replay_memory.sample(self.batch_size)\n",
    "        s, a, r, s2, d = zip(*transitions)\n",
    "        \n",
    "        state_b = torch.FloatTensor(np.array(s)).to(self.device)\n",
    "        action_b = torch.LongTensor(a).unsqueeze(1).to(self.device)\n",
    "        reward_b = torch.FloatTensor(r).unsqueeze(1).to(self.device)\n",
    "        \n",
    "        \n",
    "        next_b = torch.FloatTensor(np.array(s2)).to(self.device)\n",
    "        done_b = torch.FloatTensor(d).unsqueeze(1).to(self.device)\n",
    "\n",
    "\n",
    "        current_q = self.q_network(state_b).gather(1, action_b)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            next_actions = self.q_network(next_b).argmax(1).unsqueeze(1)\n",
    "            next_q = self.target_network(next_b).gather(1, next_actions)\n",
    "            \n",
    "            target_q = reward_b + (1 - done_b) * self.gamma * next_q\n",
    "\n",
    "\n",
    "\n",
    "        loss = nn.MSELoss()(current_q, target_q)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return float(loss.item())\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# Training and evaluation\n",
    "\n",
    "def train_agent(env, agent, num_episodes=700, max_steps=100, print_every=50):\n",
    "    \n",
    "    episode_rewards = []\n",
    "    epsilon_history = []\n",
    "    hit_rates = []\n",
    "\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        \n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0.0\n",
    "\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            \n",
    "            action = agent.select_action(state)\n",
    "            \n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            \n",
    "            agent.store_transition(state, action, reward, next_state, done)\n",
    "            \n",
    "            agent.optimize_model()\n",
    "            agent.update_epsilon()\n",
    "\n",
    "\n",
    "\n",
    "            if agent.steps % agent.target_update_freq == 0:\n",
    "                \n",
    "                agent.update_target_network()\n",
    "\n",
    "\n",
    "            state = next_state\n",
    "            \n",
    "            episode_reward += reward\n",
    "            \n",
    "            if done or truncated:\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "        episode_rewards.append(episode_reward)\n",
    "        epsilon_history.append(agent.epsilon)\n",
    "        \n",
    "        hit_rate = env.hits / max(1, env.total_accesses) * 100\n",
    "        hit_rates.append(hit_rate)\n",
    "\n",
    "\n",
    "\n",
    "        if (episode + 1) % print_every == 0 or episode == num_episodes - 1:\n",
    "            mean_reward = np.mean(episode_rewards[-print_every:])\n",
    "            mean_hit = np.mean(hit_rates[-print_every:])\n",
    "            print(f\"Ep {episode+1}/{num_episodes} | Reward (last {print_every}): {mean_reward:.2f} | Hit: {mean_hit:.2f}% | ε: {agent.epsilon:.3f}\", flush=True)\n",
    "\n",
    "\n",
    "    return episode_rewards, epsilon_history\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_agent(env, agent, pattern='mixed', num_episodes=20):\n",
    "    \n",
    "    eval_rewards = []\n",
    "    hit_rates = []\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        \n",
    "        state, _ = env.reset(options={'pattern': pattern, 'sequence_idx': i})\n",
    "        \n",
    "        episode_reward = 0.0\n",
    "        done = False\n",
    "        steps = 0\n",
    "        \n",
    "        while not done and steps < env.sequence_length:\n",
    "            \n",
    "            action = agent.select_action(state, evaluation=True)\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "            \n",
    "            if truncated:\n",
    "                break\n",
    "            \n",
    "        hit_rates.append(env.hits / max(1, env.total_accesses) * 100)\n",
    "        \n",
    "        eval_rewards.append(episode_reward)\n",
    "        \n",
    "    return eval_rewards, hit_rates\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_baseline(env, policy, pattern='mixed', num_episodes=20):\n",
    "    \n",
    "    rewards = []\n",
    "    hit_rates = []\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        \n",
    "        # reset environment and policy so no stale state\n",
    "        state, _ = env.reset(options={'pattern': pattern, 'sequence_idx': i})\n",
    "        policy.reset()\n",
    "\n",
    "\n",
    "\n",
    "        total_reward = 0.0\n",
    "        hits = 0\n",
    "        total = 0\n",
    "\n",
    "\n",
    "\n",
    "        # iterate the access sequence produced by the environment\n",
    "        \n",
    "        for addr in env.access_sequence:\n",
    "            \n",
    "            is_hit = policy.access(addr)\n",
    "            \n",
    "            \n",
    "            if is_hit:\n",
    "                hits += 1\n",
    "                total_reward += 1.0\n",
    "            else:\n",
    "                total_reward -= 1.0\n",
    "            total += 1\n",
    "\n",
    "\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "        hit_rates.append(hits / max(1, total) * 100)\n",
    "        \n",
    "    return rewards, hit_rates\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_results(dqn_rewards, ddqn_rewards, dqn_epsilon, ddqn_epsilon, results, out='cache_final_results.png'):\n",
    "    \n",
    "    \n",
    "    fig = plt.figure(figsize=(18, 9))\n",
    "    ax1 = plt.subplot(2, 3, 1)\n",
    "    \n",
    "    window = 50\n",
    "    \n",
    "    if len(dqn_rewards) >= window:\n",
    "        dqn_ma = np.convolve(dqn_rewards, np.ones(window) / window, mode='valid')\n",
    "        ax1.plot(dqn_ma, label='DQN')\n",
    "        \n",
    "    if len(ddqn_rewards) >= window:\n",
    "        ddqn_ma = np.convolve(ddqn_rewards, np.ones(window) / window, mode='valid')\n",
    "        ax1.plot(ddqn_ma, label='Double DQN')\n",
    "        \n",
    "        \n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Reward (moving avg)')\n",
    "    \n",
    "    ax1.set_title('Training Rewards')\n",
    "    ax1.legend()\n",
    "    \n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    ax2 = plt.subplot(2, 3, 2)\n",
    "    ax2.plot(dqn_epsilon, label='DQN')\n",
    "    ax2.plot(ddqn_epsilon, label='Double DQN')\n",
    "    \n",
    "    ax2.set_xlabel('Episode')\n",
    "    ax2.set_ylabel('Epsilon')\n",
    "    ax2.set_title('Exploration Rate')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "    for idx, pattern in enumerate(['easy', 'medium', 'hard', 'mixed']):\n",
    "        \n",
    "        ax = plt.subplot(2, 3, idx + 3)\n",
    "        methods = ['LRU', 'FIFO', 'LFU', 'DQN', 'DDQN']\n",
    "        \n",
    "        means = [results[pattern][m][0] for m in methods]\n",
    "        stds = [results[pattern][m][1] for m in methods]\n",
    "        \n",
    "        bars = ax.bar(methods, means, yerr=stds, capsize=5, alpha=0.8)\n",
    "        ax.set_ylabel('Hit Rate (%)')\n",
    "        \n",
    "        ax.set_title(f'{pattern.capitalize()} Pattern')\n",
    "        ax.set_ylim(0, 100)\n",
    "        ax.grid(True, axis='y', alpha=0.3)\n",
    "        \n",
    "        \n",
    "        for bar, mean in zip(bars, means):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width() / 2., height + 1.0, f'{mean:.1f}%',\n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.close(fig)\n",
    "    print(f\"✓ Results saved to '{out}'\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_cache_comparison(num_episodes=700):\n",
    "    env = CacheEnv(cache_size=8, memory_size=64, sequence_length=100)\n",
    "    \n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "\n",
    "\n",
    "    print(\"CACHE REPLACEMENT RL - TRAINING\")\n",
    "   \n",
    "    \n",
    "    print(f\"Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")\n",
    "    print(f\"Cache: {env.cache_size} | Memory: {env.memory_size} | Seq: {env.sequence_length}\")\n",
    "    \n",
    "    print(\"=\" * 60, flush=True)\n",
    "\n",
    "    dqn_agent = DQNAgent(state_dim, action_dim, learning_rate=1e-3, gamma=0.95,\n",
    "                         epsilon_start=1.0, epsilon_end=0.01, epsilon_decay_steps=50000,\n",
    "                         replay_memory_size=30000, batch_size=64, target_update_freq=300)\n",
    "    dqn_rewards, dqn_epsilon = train_agent(env, dqn_agent, num_episodes, max_steps=env.sequence_length, print_every=50)\n",
    "\n",
    "    ddqn_agent = DoubleDQNAgent(state_dim, action_dim, learning_rate=1e-3, gamma=0.95,\n",
    "                                epsilon_start=1.0, epsilon_end=0.01, epsilon_decay_steps=50000,\n",
    "                                replay_memory_size=30000, batch_size=64, target_update_freq=300)\n",
    "    ddqn_rewards, ddqn_epsilon = train_agent(env, ddqn_agent, num_episodes, max_steps=env.sequence_length, print_every=50)\n",
    "\n",
    "\n",
    "    results = {}\n",
    "    \n",
    "    for pattern in ['easy', 'medium', 'hard', 'mixed']:\n",
    "        \n",
    "        lru_eval, lru_hits = evaluate_baseline(env, LRUPolicy(env.cache_size), pattern)\n",
    "        fifo_eval, fifo_hits = evaluate_baseline(env, FIFOPolicy(env.cache_size), pattern)\n",
    "        \n",
    "        lfu_eval, lfu_hits = evaluate_baseline(env, LFUPolicy(env.cache_size), pattern)\n",
    "        dqn_eval, dqn_hits = evaluate_agent(env, dqn_agent, pattern)\n",
    "        ddqn_eval, ddqn_hits = evaluate_agent(env, ddqn_agent, pattern)\n",
    "\n",
    "\n",
    "        lru_mean, lru_std = np.mean(lru_hits), np.std(lru_hits)\n",
    "        fifo_mean, fifo_std = np.mean(fifo_hits), np.std(fifo_hits)\n",
    "        \n",
    "        lfu_mean, lfu_std = np.mean(lfu_hits), np.std(lfu_hits)\n",
    "        \n",
    "        dqn_mean, dqn_std = np.mean(dqn_hits), np.std(dqn_hits)\n",
    "        ddqn_mean, ddqn_std = np.mean(ddqn_hits), np.std(ddqn_hits)\n",
    "\n",
    "\n",
    "        best_baseline = max(lru_mean, fifo_mean, lfu_mean)\n",
    "\n",
    "\n",
    "        print(f\"\\n{pattern.upper()} Pattern:\")\n",
    "        \n",
    "        print(f\" LRU:  {lru_mean:.2f}% ± {lru_std:.2f}%\")\n",
    "        print(f\" FIFO: {fifo_mean:.2f}% ± {fifo_std:.2f}%\")\n",
    "        \n",
    "        print(f\" LFU:  {lfu_mean:.2f}% ± {lfu_std:.2f}%\")\n",
    "        print(f\" DQN:  {dqn_mean:.2f}% ± {dqn_std:.2f}%  {'✓' if dqn_mean > best_baseline else '✗'} ({dqn_mean - best_baseline:+.2f}%)\")\n",
    "        print(f\" DDQN: {ddqn_mean:.2f}% ± {ddqn_std:.2f}%  {'✓' if ddqn_mean > best_baseline else '✗'} ({ddqn_mean - best_baseline:+.2f}%)\")\n",
    "\n",
    "\n",
    "        results[pattern] = {\n",
    "            'LRU': (lru_mean, lru_std),\n",
    "            'FIFO': (fifo_mean, fifo_std),\n",
    "            'LFU': (lfu_mean, lfu_std),\n",
    "            'DQN': (dqn_mean, dqn_std),\n",
    "            'DDQN': (ddqn_mean, ddqn_std)\n",
    "        }\n",
    "\n",
    "\n",
    "    plot_results(dqn_rewards, ddqn_rewards, dqn_epsilon, ddqn_epsilon, results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"Starting training...\", flush=True)\n",
    "    \n",
    "    results = train_cache_comparison(num_episodes=700)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### I also made attempts to make model using PPO and A2C RL algorithms but due to the complex caching environment and \n",
    "##### complex memory caching task, it proved to be very challenging to create or integrate a A2C or PPO that could learn \n",
    "##### and converge properly to a desired result.\n",
    "\n",
    "\"\"\"\n",
    "Attempts at developing PPO and A2C that were not converging properly for given complex tasks:\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "    \n",
    "    \n",
    "        super().__init__()\n",
    "        \n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Linear(hidden_dim, action_dim)\n",
    "        self.critic = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "    \n",
    "        x = self.shared(x)\n",
    "        return self.actor(x), self.critic(x)\n",
    "        \n",
    "        \n",
    "\n",
    "class A2CAgent:\n",
    "\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.95):\n",
    "    \n",
    "    \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.model = ActorCritic(state_dim, action_dim).to(self.device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def select_action(self, state):\n",
    "    \n",
    "    \n",
    "        state_t = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        logits, _ = self.model(state_t)\n",
    "        \n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        \n",
    "        \n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action)\n",
    "        \n",
    "        \n",
    "    def update(self, trajectories):\n",
    "    \n",
    "        R = 0\n",
    "        returns = []\n",
    "        \n",
    "        for r, _, _ in reversed(trajectories):\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "            \n",
    "        returns = torch.FloatTensor(returns).to(self.device)\n",
    "        \n",
    "        \n",
    "        states = torch.FloatTensor([s for _, s, _ in trajectories]).to(self.device)\n",
    "        actions = torch.LongTensor([a for _, _, a in trajectories]).to(self.device)\n",
    "        \n",
    "        log_probs = torch.stack([lp for lp, _, _ in trajectories])\n",
    "        \n",
    "        logits, values = self.model(states)\n",
    "        values = values.squeeze()\n",
    "        \n",
    "        advantage = returns - values.detach()\n",
    "        policy_loss = -(log_probs * advantage).mean()\n",
    "        \n",
    "        value_loss = nn.MSELoss()(values, returns)\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        (policy_loss + value_loss).backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "class PPOAgent:\n",
    "\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.95, eps_clip=0.2):\n",
    "    \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        \n",
    "        self.model = ActorCritic(state_dim, action_dim).to(self.device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def select_action(self, state):\n",
    "    \n",
    "        state_t = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        logits, _ = self.model(state_t)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        return action.item(), dist.log_prob(action)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def update(self, trajectories, epochs=4):\n",
    "    \n",
    "        states = torch.FloatTensor([s for _, s, _ in trajectories]).to(self.device)\n",
    "        actions = torch.LongTensor([a for _, _, a in trajectories]).to(self.device)\n",
    "        \n",
    "        old_log_probs = torch.stack([lp for lp, _, _ in trajectories]).detach()\n",
    "        \n",
    "        \n",
    "        returns = []\n",
    "        \n",
    "        R = 0\n",
    "        \n",
    "        for r, _, _ in reversed(trajectories):\n",
    "        \n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "            \n",
    "        returns = torch.FloatTensor(returns).to(self.device)\n",
    "        \n",
    "        \n",
    "        for _ in range(epochs):\n",
    "        \n",
    "            logits, values = self.model(states)\n",
    "            values = values.squeeze()\n",
    "            \n",
    "            \n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            \n",
    "            log_probs = dist.log_prob(actions)\n",
    "            ratio = torch.exp(log_probs - old_log_probs)\n",
    "            advantage = returns - values.detach()\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1.0 - self.eps_clip, 1.0 + self.eps_clip) * advantage\n",
    "\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            value_loss = nn.MSELoss()(values, returns)\n",
    "            self.optimizer.zero_grad()\n",
    "            (policy_loss + value_loss).backward()\n",
    "            \n",
    "            self.optimizer.step()\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "References\n",
    "\n",
    "https://www.cs.utexas.edu/~lin/papers/micro19c.pdf\n",
    "\n",
    "https://www.cs.cmu.edu/~weinaw/pdf/delayed-hits.pdf\n",
    "\n",
    "https://gymnasium.farama.org/index.html\n",
    "\n",
    "https://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
